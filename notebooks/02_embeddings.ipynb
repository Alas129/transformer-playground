{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Embeddings and Positional Encoding\n",
        "\n",
        "## Converting Text to Numbers\n",
        "\n",
        "Neural networks work with numbers, not words. This notebook covers:\n",
        "\n",
        "1. **Token Embeddings**: Converting words/characters to vectors\n",
        "2. **Positional Encoding**: Telling the model where each token is\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Tokenization\n",
        "\n",
        "Before embedding, we need to split text into tokens. There are several strategies:\n",
        "\n",
        "- **Character-level**: `\"hello\"` → `['h', 'e', 'l', 'l', 'o']`\n",
        "- **Word-level**: `\"hello world\"` → `['hello', 'world']`\n",
        "- **Subword (BPE)**: `\"unhappiness\"` → `['un', 'happiness']`\n",
        "\n",
        "We'll use **character-level** for simplicity (easier to understand, smaller vocabulary).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharTokenizer:\n",
        "    \"\"\"Simple character-level tokenizer.\"\"\"\n",
        "    \n",
        "    def __init__(self, text):\n",
        "        # Get unique characters\n",
        "        chars = sorted(set(text))\n",
        "        \n",
        "        # Create mappings\n",
        "        self.char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "        self.idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "        self.vocab_size = len(chars)\n",
        "    \n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to list of integers.\"\"\"\n",
        "        return [self.char_to_idx[c] for c in text]\n",
        "    \n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of integers back to text.\"\"\"\n",
        "        return ''.join(self.idx_to_char[i] for i in indices)\n",
        "\n",
        "# Example\n",
        "sample_text = \"Hello, World!\"\n",
        "tokenizer = CharTokenizer(sample_text)\n",
        "\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"\\nCharacter to Index mapping:\")\n",
        "for char, idx in tokenizer.char_to_idx.items():\n",
        "    display_char = repr(char) if char in ' \\n\\t' else char\n",
        "    print(f\"  '{display_char}' → {idx}\")\n",
        "\n",
        "encoded = tokenizer.encode(sample_text)\n",
        "print(f\"\\nEncoded: {encoded}\")\n",
        "print(f\"Decoded: '{tokenizer.decode(encoded)}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Token Embeddings\n",
        "\n",
        "Now we convert token indices to **dense vectors**.\n",
        "\n",
        "### Why not one-hot encoding?\n",
        "\n",
        "One-hot: Each token is a vector with a single 1 and rest 0s.\n",
        "\n",
        "```\n",
        "Vocab: [a, b, c, d]\n",
        "'a' → [1, 0, 0, 0]\n",
        "'b' → [0, 1, 0, 0]\n",
        "```\n",
        "\n",
        "Problems:\n",
        "1. **Huge vectors**: Vocab of 50,000 → 50,000-dim vectors!\n",
        "2. **No similarity**: `cat` and `kitten` are equally different as `cat` and `airplane`\n",
        "\n",
        "### Embeddings: Dense, learnable vectors\n",
        "\n",
        "```\n",
        "'a' → [0.2, -0.5, 0.8, 0.1]  (small, dense)\n",
        "'b' → [0.3, 0.1, -0.2, 0.9]\n",
        "```\n",
        "\n",
        "Similar words get similar vectors through training!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TokenEmbedding:\n",
        "    \"\"\"\n",
        "    Convert token indices to dense vectors.\n",
        "    \n",
        "    This is essentially a lookup table.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        vocab_size: number of unique tokens\n",
        "        embed_dim: dimension of embedding vectors\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Initialize embedding matrix randomly\n",
        "        # Shape: (vocab_size, embed_dim)\n",
        "        # Each row is the embedding for one token\n",
        "        self.embeddings = np.random.randn(vocab_size, embed_dim) * 0.1\n",
        "    \n",
        "    def forward(self, indices):\n",
        "        \"\"\"\n",
        "        Look up embeddings for given indices.\n",
        "        \n",
        "        indices: list or array of token indices\n",
        "        Returns: (seq_len, embed_dim) array of embeddings\n",
        "        \"\"\"\n",
        "        return self.embeddings[indices]\n",
        "\n",
        "# Example\n",
        "embed_dim = 8\n",
        "embedding = TokenEmbedding(tokenizer.vocab_size, embed_dim)\n",
        "\n",
        "# Embed our sample text\n",
        "indices = tokenizer.encode(\"Hello\")\n",
        "embedded = embedding.forward(indices)\n",
        "\n",
        "print(f\"Input text: 'Hello'\")\n",
        "print(f\"Token indices: {indices}\")\n",
        "print(f\"\\nEmbedding shape: {embedded.shape}\")\n",
        "print(f\"(5 tokens, each with {embed_dim}-dim embedding)\")\n",
        "print(f\"\\nEmbedding for each character:\")\n",
        "for i, (char, vec) in enumerate(zip('Hello', embedded)):\n",
        "    print(f\"  '{char}' → [{', '.join(f'{v:6.3f}' for v in vec)}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the embedding matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "im = ax.imshow(embedding.embeddings, cmap='RdBu', aspect='auto')\n",
        "ax.set_xlabel('Embedding Dimension', fontsize=12)\n",
        "ax.set_ylabel('Token Index', fontsize=12)\n",
        "ax.set_title('Token Embedding Matrix\\n(Each row is one token\\'s embedding)', fontsize=14)\n",
        "\n",
        "# Add character labels\n",
        "ax.set_yticks(range(tokenizer.vocab_size))\n",
        "ax.set_yticklabels([f\"{i}: '{tokenizer.idx_to_char[i]}'\" for i in range(tokenizer.vocab_size)])\n",
        "\n",
        "plt.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Positional Encoding\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Unlike RNNs, Transformers process all tokens simultaneously. But the same word in different positions should mean different things:\n",
        "\n",
        "- \"The cat chased the dog\" vs \"The dog chased the cat\"\n",
        "\n",
        "The embedding for \"cat\" is the same in both cases! We need to add position information.\n",
        "\n",
        "### The Solution: Sinusoidal Positional Encoding\n",
        "\n",
        "Add a unique pattern to each position using sine and cosine waves.\n",
        "\n",
        "Where:\n",
        "- `pos` = position in sequence (0, 1, 2, ...)\n",
        "- `i` = dimension index\n",
        "- `d_model` = embedding dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding:\n",
        "    \"\"\"\n",
        "    Add positional information using sinusoidal functions.\n",
        "    \n",
        "    Why sin/cos?\n",
        "    1. Bounded values (-1 to 1)\n",
        "    2. Different frequencies capture different position scales\n",
        "    3. Can extrapolate to unseen sequence lengths\n",
        "    4. Relative positions are easy to compute\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_seq_len, embed_dim):\n",
        "        \"\"\"\n",
        "        max_seq_len: maximum sequence length to support\n",
        "        embed_dim: dimension of embeddings (must match token embeddings)\n",
        "        \"\"\"\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = np.zeros((max_seq_len, embed_dim))\n",
        "        \n",
        "        # Position indices: [0, 1, 2, ..., max_seq_len-1]\n",
        "        position = np.arange(max_seq_len).reshape(-1, 1)\n",
        "        \n",
        "        # Dimension indices for the formula\n",
        "        # div_term = 10000^(2i/d_model)\n",
        "        div_term = np.exp(np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
        "        \n",
        "        # Apply sin to even indices, cos to odd indices\n",
        "        pe[:, 0::2] = np.sin(position * div_term)  # Even dimensions\n",
        "        pe[:, 1::2] = np.cos(position * div_term)  # Odd dimensions\n",
        "        \n",
        "        self.pe = pe\n",
        "    \n",
        "    def forward(self, seq_len):\n",
        "        \"\"\"\n",
        "        Get positional encodings for a sequence.\n",
        "        \n",
        "        seq_len: length of the sequence\n",
        "        Returns: (seq_len, embed_dim) positional encodings\n",
        "        \"\"\"\n",
        "        return self.pe[:seq_len]\n",
        "\n",
        "# Create positional encoding\n",
        "max_seq_len = 100\n",
        "pos_encoder = PositionalEncoding(max_seq_len, embed_dim)\n",
        "\n",
        "# Get encodings for first 10 positions\n",
        "pe = pos_encoder.forward(10)\n",
        "\n",
        "print(f\"Positional Encoding shape: {pe.shape}\")\n",
        "print(f\"\\nFirst 5 positions:\")\n",
        "for i in range(5):\n",
        "    print(f\"  Position {i}: [{', '.join(f'{v:6.3f}' for v in pe[i])}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize positional encodings\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Full matrix view\n",
        "ax = axes[0]\n",
        "pe_full = pos_encoder.forward(50)\n",
        "im = ax.imshow(pe_full, cmap='RdBu', aspect='auto')\n",
        "ax.set_xlabel('Embedding Dimension', fontsize=12)\n",
        "ax.set_ylabel('Position', fontsize=12)\n",
        "ax.set_title('Positional Encoding Matrix\\n(Each row is unique)', fontsize=14)\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# Individual dimension waves\n",
        "ax = axes[1]\n",
        "positions = np.arange(50)\n",
        "for dim in [0, 2, 4, 6]:\n",
        "    ax.plot(positions, pe_full[:, dim], label=f'Dim {dim}', lw=2)\n",
        "ax.set_xlabel('Position', fontsize=12)\n",
        "ax.set_ylabel('Encoding Value', fontsize=12)\n",
        "ax.set_title('Sinusoidal Waves at Different Dimensions\\n(Lower dims = higher frequency)', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice: Lower dimensions oscillate faster (high frequency).\")\n",
        "print(\"This lets the model capture both fine and coarse position info.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Combining Token + Position\n",
        "\n",
        "The final input to the Transformer is:\n",
        "\n",
        "**Input = TokenEmbedding(x) + PositionalEncoding(pos)**\n",
        "\n",
        "Simply **add** the position vectors to the token vectors!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEmbedding:\n",
        "    \"\"\"\n",
        "    Complete embedding layer for Transformer.\n",
        "    Combines token embeddings with positional encodings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_dim, max_seq_len):\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(max_seq_len, embed_dim)\n",
        "        self.embed_dim = embed_dim\n",
        "    \n",
        "    def forward(self, token_indices):\n",
        "        \"\"\"\n",
        "        Convert token indices to embeddings with position info.\n",
        "        \n",
        "        token_indices: list of token indices\n",
        "        Returns: (seq_len, embed_dim) tensor ready for attention\n",
        "        \"\"\"\n",
        "        seq_len = len(token_indices)\n",
        "        \n",
        "        # Get token embeddings\n",
        "        tok_emb = self.token_embedding.forward(token_indices)\n",
        "        \n",
        "        # Get positional encodings\n",
        "        pos_enc = self.pos_encoding.forward(seq_len)\n",
        "        \n",
        "        # Add them together\n",
        "        # (Often scaled by sqrt(embed_dim) for stability)\n",
        "        return tok_emb * np.sqrt(self.embed_dim) + pos_enc\n",
        "\n",
        "# Example: Full embedding pipeline\n",
        "text = \"Hello, World!\"\n",
        "tokenizer = CharTokenizer(text)\n",
        "transformer_emb = TransformerEmbedding(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_dim=16,\n",
        "    max_seq_len=100\n",
        ")\n",
        "\n",
        "# Convert text to embeddings\n",
        "indices = tokenizer.encode(text)\n",
        "embeddings = transformer_emb.forward(indices)\n",
        "\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Token indices: {indices}\")\n",
        "print(f\"Final embedding shape: {embeddings.shape}\")\n",
        "print(f\"\\nThese embeddings are ready for self-attention!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the complete embedding process\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Token embeddings only\n",
        "tok_emb = transformer_emb.token_embedding.forward(indices) * np.sqrt(transformer_emb.embed_dim)\n",
        "ax = axes[0]\n",
        "im = ax.imshow(tok_emb, cmap='RdBu', aspect='auto')\n",
        "ax.set_yticks(range(len(text)))\n",
        "ax.set_yticklabels(list(text))\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_title('Token Embeddings Only\\n(No position info)')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# Positional encodings only\n",
        "pos_enc = transformer_emb.pos_encoding.forward(len(text))\n",
        "ax = axes[1]\n",
        "im = ax.imshow(pos_enc, cmap='RdBu', aspect='auto')\n",
        "ax.set_yticks(range(len(text)))\n",
        "ax.set_yticklabels([f'pos {i}' for i in range(len(text))])\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_title('Positional Encodings Only\\n(No token info)')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# Combined\n",
        "ax = axes[2]\n",
        "im = ax.imshow(embeddings, cmap='RdBu', aspect='auto')\n",
        "ax.set_yticks(range(len(text)))\n",
        "ax.set_yticklabels(list(text))\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_title('Token + Position (Combined)\\n(Ready for attention!)')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### The Embedding Pipeline\n",
        "\n",
        "```\n",
        "Text: \"Hello\"\n",
        "    | Tokenization\n",
        "Tokens: [H, e, l, l, o]\n",
        "    | Token IDs\n",
        "Indices: [7, 4, 11, 11, 14]\n",
        "    | Embedding lookup\n",
        "Token Embeddings: (5, d_model)\n",
        "    + \n",
        "Positional Encoding: (5, d_model)\n",
        "    |\n",
        "Final Input: (5, d_model)  <-- Ready for attention!\n",
        "```\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Token embeddings** map discrete tokens to continuous vectors\n",
        "2. **Positional encoding** adds position information (since no recurrence)\n",
        "3. **Sinusoidal** encodings use fixed sin/cos patterns\n",
        "4. **Learnable** embeddings are trained with the model\n",
        "5. **Final input** = Token Embedding + Positional Encoding\n",
        "\n",
        "---\n",
        "\n",
        "**Next: 03_attention.ipynb** - The core innovation: Self-Attention!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

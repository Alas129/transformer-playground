{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Self-Attention - The Core Innovation\n",
        "\n",
        "## \"Attention Is All You Need\"\n",
        "\n",
        "Self-attention is **the** key mechanism that makes Transformers work. In this notebook, we'll:\n",
        "\n",
        "1. Understand what attention does intuitively\n",
        "2. Implement scaled dot-product attention from scratch\n",
        "3. Visualize attention patterns\n",
        "4. Learn about masking for text generation\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Intuition: Query, Key, Value\n",
        "\n",
        "Think of attention like a **search system**:\n",
        "\n",
        "- **Query (Q)**: \"What am I looking for?\"\n",
        "- **Key (K)**: \"What does each position contain?\"\n",
        "- **Value (V)**: \"What information should I retrieve?\"\n",
        "\n",
        "### Example: Understanding a Sentence\n",
        "\n",
        "```\n",
        "Sentence: \"The cat sat on the mat because it was tired\"\n",
        "\n",
        "When processing \"it\":\n",
        "  Query: \"What does 'it' refer to?\"\n",
        "  Keys:  Each word's \"identifier\"\n",
        "  Values: Each word's meaning\n",
        "  \n",
        "Result: High attention on \"cat\" (the referent of \"it\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Computing Q, K, V\n",
        "\n",
        "In self-attention, Q, K, and V all come from the **same** input, but are transformed differently:\n",
        "\n",
        "```\n",
        "Input X (seq_len, d_model)\n",
        "    |\n",
        "    +---> Q = X @ W_Q  (Query projection)\n",
        "    |\n",
        "    +---> K = X @ W_K  (Key projection)  \n",
        "    |\n",
        "    +---> V = X @ W_V  (Value projection)\n",
        "```\n",
        "\n",
        "The weight matrices W_Q, W_K, W_V are learned during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample input: a sequence of 4 tokens, each with 8-dim embedding\n",
        "seq_len = 4\n",
        "d_model = 8\n",
        "d_k = 8  # dimension of keys/queries (often same as d_model)\n",
        "\n",
        "# Input embeddings (pretend these came from the embedding layer)\n",
        "X = np.random.randn(seq_len, d_model)\n",
        "\n",
        "# Learnable weight matrices\n",
        "W_Q = np.random.randn(d_model, d_k) * 0.1\n",
        "W_K = np.random.randn(d_model, d_k) * 0.1\n",
        "W_V = np.random.randn(d_model, d_k) * 0.1\n",
        "\n",
        "# Compute Q, K, V\n",
        "Q = X @ W_Q  # (seq_len, d_k)\n",
        "K = X @ W_K  # (seq_len, d_k)\n",
        "V = X @ W_V  # (seq_len, d_k)\n",
        "\n",
        "print(\"Input X shape:\", X.shape)\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"K shape:\", K.shape)\n",
        "print(\"V shape:\", V.shape)\n",
        "print(\"\\nQ, K, V have the same shape but contain different information!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Computing Attention Scores\n",
        "\n",
        "How similar is each Query to each Key?\n",
        "\n",
        "**Attention Scores = Q @ K^T**\n",
        "\n",
        "This gives us a (seq_len, seq_len) matrix where entry [i, j] tells us how much position i should attend to position j.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute attention scores\n",
        "scores = Q @ K.T  # (seq_len, seq_len)\n",
        "\n",
        "print(\"Attention scores shape:\", scores.shape)\n",
        "print(\"\\nRaw attention scores:\")\n",
        "print(scores.round(2))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "im = ax.imshow(scores, cmap='RdBu')\n",
        "ax.set_xlabel('Key Position (attending TO)')\n",
        "ax.set_ylabel('Query Position (attending FROM)')\n",
        "ax.set_title('Raw Attention Scores\\n(Before scaling and softmax)')\n",
        "ax.set_xticks(range(seq_len))\n",
        "ax.set_yticks(range(seq_len))\n",
        "plt.colorbar(im)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Scaling\n",
        "\n",
        "### Why Scale?\n",
        "\n",
        "When d_k is large, the dot products can get very large. Large values cause softmax to produce very peaked distributions (almost one-hot), which:\n",
        "1. Kills gradients (vanishing gradient problem)\n",
        "2. Makes the model too \"confident\" too early\n",
        "\n",
        "**Solution**: Divide by sqrt(d_k)\n",
        "\n",
        "```\n",
        "Scaled Scores = (Q @ K^T) / sqrt(d_k)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the scores\n",
        "scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "print(f\"Scaling factor: sqrt({d_k}) = {np.sqrt(d_k):.2f}\")\n",
        "print(f\"\\nBefore scaling - max: {scores.max():.2f}, min: {scores.min():.2f}\")\n",
        "print(f\"After scaling  - max: {scaled_scores.max():.2f}, min: {scaled_scores.min():.2f}\")\n",
        "\n",
        "# Compare softmax distributions\n",
        "def softmax(x, axis=-1):\n",
        "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Without scaling\n",
        "weights_unscaled = softmax(scores * 3)  # Exaggerate to show the effect\n",
        "ax = axes[0]\n",
        "ax.imshow(weights_unscaled, cmap='Blues')\n",
        "ax.set_title('Softmax WITHOUT proper scaling\\n(Too peaked/confident)')\n",
        "ax.set_xlabel('Key Position')\n",
        "ax.set_ylabel('Query Position')\n",
        "\n",
        "# With scaling\n",
        "weights_scaled = softmax(scaled_scores)\n",
        "ax = axes[1]\n",
        "ax.imshow(weights_scaled, cmap='Blues')\n",
        "ax.set_title('Softmax WITH scaling\\n(Smoother, better gradients)')\n",
        "ax.set_xlabel('Key Position')\n",
        "ax.set_ylabel('Query Position')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Softmax - Converting to Probabilities\n",
        "\n",
        "Apply softmax to convert scores to attention weights (probabilities that sum to 1 for each query).\n",
        "\n",
        "**Attention Weights = softmax(Scaled Scores)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply softmax to get attention weights\n",
        "attention_weights = softmax(scaled_scores)\n",
        "\n",
        "print(\"Attention weights (each row sums to 1):\")\n",
        "print(attention_weights.round(3))\n",
        "print(\"\\nRow sums:\", attention_weights.sum(axis=1).round(3))\n",
        "\n",
        "# Visualize with actual values\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "im = ax.imshow(attention_weights, cmap='Blues')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        text = ax.text(j, i, f'{attention_weights[i, j]:.2f}',\n",
        "                       ha='center', va='center', fontsize=10)\n",
        "\n",
        "ax.set_xlabel('Key Position (attending TO)')\n",
        "ax.set_ylabel('Query Position (attending FROM)')\n",
        "ax.set_title('Attention Weights\\n(Each row sums to 1)')\n",
        "ax.set_xticks(range(seq_len))\n",
        "ax.set_yticks(range(seq_len))\n",
        "ax.set_xticklabels([f'K{i}' for i in range(seq_len)])\n",
        "ax.set_yticklabels([f'Q{i}' for i in range(seq_len)])\n",
        "plt.colorbar(im)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Computing the Output\n",
        "\n",
        "Multiply attention weights by Values to get the final output.\n",
        "\n",
        "**Output = Attention Weights @ V**\n",
        "\n",
        "Each output position is a weighted sum of all Value vectors, where the weights are the attention weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute attention output\n",
        "attention_output = attention_weights @ V\n",
        "\n",
        "print(\"V (Values) shape:\", V.shape)\n",
        "print(\"Attention weights shape:\", attention_weights.shape)\n",
        "print(\"Output shape:\", attention_output.shape)\n",
        "\n",
        "print(\"\\n--- What happened ---\")\n",
        "print(\"Each output row is a weighted combination of ALL value rows.\")\n",
        "print(\"The weights come from the attention pattern we just computed.\")\n",
        "\n",
        "# Show the computation for position 0\n",
        "print(f\"\\nOutput[0] = \", end=\"\")\n",
        "for i in range(seq_len):\n",
        "    print(f\"{attention_weights[0, i]:.2f}*V[{i}]\", end=\"\")\n",
        "    if i < seq_len - 1:\n",
        "        print(\" + \", end=\"\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Scaled Dot-Product Attention\n",
        "\n",
        "Let's put it all together in one function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention.\n",
        "    \n",
        "    Args:\n",
        "        Q: Queries (seq_len, d_k)\n",
        "        K: Keys (seq_len, d_k)\n",
        "        V: Values (seq_len, d_v)\n",
        "        mask: Optional mask (seq_len, seq_len)\n",
        "        \n",
        "    Returns:\n",
        "        output: Attended values (seq_len, d_v)\n",
        "        attention_weights: Attention pattern (seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    d_k = K.shape[-1]\n",
        "    \n",
        "    # Step 1: Compute attention scores\n",
        "    scores = Q @ K.T  # (seq_len, seq_len)\n",
        "    \n",
        "    # Step 2: Scale\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "    \n",
        "    # Step 3: Apply mask (optional - for causal attention)\n",
        "    if mask is not None:\n",
        "        scaled_scores = np.where(mask == 0, -1e9, scaled_scores)\n",
        "    \n",
        "    # Step 4: Softmax\n",
        "    attention_weights = softmax(scaled_scores, axis=-1)\n",
        "    \n",
        "    # Step 5: Weighted sum of values\n",
        "    output = attention_weights @ V\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Test our function\n",
        "output, weights = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention weights shape:\", weights.shape)\n",
        "print(\"\\nSuccess! Our attention function works.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Causal Masking - For Text Generation\n",
        "\n",
        "When generating text, we can only look at **past** tokens, not future ones.\n",
        "\n",
        "**Problem**: Self-attention looks at ALL positions by default.\n",
        "\n",
        "**Solution**: Mask out future positions with -infinity before softmax.\n",
        "\n",
        "```\n",
        "Without mask:          With causal mask:\n",
        "[see see see see]      [see  -inf -inf -inf]\n",
        "[see see see see]  =>  [see  see  -inf -inf]\n",
        "[see see see see]      [see  see  see  -inf]\n",
        "[see see see see]      [see  see  see  see ]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_causal_mask(seq_len):\n",
        "    \"\"\"\n",
        "    Create a causal mask that prevents attending to future positions.\n",
        "    \n",
        "    Returns: mask where 1 = can attend, 0 = cannot attend\n",
        "    \"\"\"\n",
        "    # Lower triangular matrix\n",
        "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
        "    return mask\n",
        "\n",
        "# Create and visualize causal mask\n",
        "causal_mask = create_causal_mask(6)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# The mask\n",
        "ax = axes[0]\n",
        "ax.imshow(causal_mask, cmap='Greens')\n",
        "ax.set_title('Causal Mask\\n(1=attend, 0=block)')\n",
        "for i in range(6):\n",
        "    for j in range(6):\n",
        "        ax.text(j, i, int(causal_mask[i, j]), ha='center', va='center')\n",
        "ax.set_xlabel('Position')\n",
        "ax.set_ylabel('Position')\n",
        "\n",
        "# Attention without mask\n",
        "_, weights_no_mask = scaled_dot_product_attention(\n",
        "    np.random.randn(6, 8), np.random.randn(6, 8), np.random.randn(6, 8)\n",
        ")\n",
        "ax = axes[1]\n",
        "ax.imshow(weights_no_mask, cmap='Blues')\n",
        "ax.set_title('Attention WITHOUT Mask\\n(Can see everything)')\n",
        "ax.set_xlabel('Attending TO')\n",
        "ax.set_ylabel('Attending FROM')\n",
        "\n",
        "# Attention with mask\n",
        "_, weights_with_mask = scaled_dot_product_attention(\n",
        "    np.random.randn(6, 8), np.random.randn(6, 8), np.random.randn(6, 8),\n",
        "    mask=causal_mask\n",
        ")\n",
        "ax = axes[2]\n",
        "ax.imshow(weights_with_mask, cmap='Blues')\n",
        "ax.set_title('Attention WITH Causal Mask\\n(Can only see past)')\n",
        "ax.set_xlabel('Attending TO')\n",
        "ax.set_ylabel('Attending FROM')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice: With the causal mask, position 0 can only attend to itself,\")\n",
        "print(\"position 1 can attend to 0 and 1, etc. No cheating by looking ahead!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Attention on Real Text\n",
        "\n",
        "Let's see how attention might work on actual words:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate attention on a real sentence\n",
        "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "seq_len = len(sentence)\n",
        "d_model = 16\n",
        "\n",
        "# Create fake embeddings and attention\n",
        "np.random.seed(123)  # For reproducibility\n",
        "X = np.random.randn(seq_len, d_model)\n",
        "\n",
        "# Initialize weights\n",
        "W_Q = np.random.randn(d_model, d_model) * 0.1\n",
        "W_K = np.random.randn(d_model, d_model) * 0.1\n",
        "W_V = np.random.randn(d_model, d_model) * 0.1\n",
        "\n",
        "Q = X @ W_Q\n",
        "K = X @ W_K\n",
        "V = X @ W_V\n",
        "\n",
        "# Compute attention\n",
        "_, attention = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(attention, cmap='Blues')\n",
        "\n",
        "# Labels\n",
        "ax.set_xticks(range(seq_len))\n",
        "ax.set_yticks(range(seq_len))\n",
        "ax.set_xticklabels(sentence)\n",
        "ax.set_yticklabels(sentence)\n",
        "ax.set_xlabel('Attending TO', fontsize=12)\n",
        "ax.set_ylabel('Attending FROM', fontsize=12)\n",
        "ax.set_title('Self-Attention on \"The cat sat on the mat\"', fontsize=14)\n",
        "\n",
        "# Add percentage annotations\n",
        "for i in range(seq_len):\n",
        "    for j in range(seq_len):\n",
        "        ax.text(j, i, f'{attention[i,j]:.0%}', ha='center', va='center', \n",
        "                fontsize=9, color='white' if attention[i,j] > 0.3 else 'black')\n",
        "\n",
        "plt.colorbar(im, label='Attention Weight')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"In a trained model, you'd see meaningful patterns like:\")\n",
        "print(\"- 'sat' attending strongly to 'cat' (subject)\")\n",
        "print(\"- 'mat' attending to 'on' (preposition)\")\n",
        "print(\"- 'the' (second one) attending to 'the' (first one)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Scaled Dot-Product Attention\n",
        "\n",
        "```\n",
        "        Q (Queries)    K (Keys)      V (Values)\n",
        "            |             |              |\n",
        "            +------+------+              |\n",
        "                   |                     |\n",
        "            scores = Q @ K.T             |\n",
        "                   |                     |\n",
        "            scaled = scores / sqrt(d_k)  |\n",
        "                   |                     |\n",
        "            [optional: apply mask]       |\n",
        "                   |                     |\n",
        "            weights = softmax(scaled)    |\n",
        "                   |                     |\n",
        "                   +----------+----------+\n",
        "                              |\n",
        "                       output = weights @ V\n",
        "```\n",
        "\n",
        "### Key Equations\n",
        "\n",
        "1. **Scores**: `Q @ K.T`\n",
        "2. **Scaling**: `/ sqrt(d_k)`\n",
        "3. **Masking**: Replace blocked positions with -inf\n",
        "4. **Softmax**: Convert to probabilities\n",
        "5. **Output**: `attention_weights @ V`\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Self-attention** lets every position look at every other position\n",
        "2. **Scaling** prevents vanishing gradients in softmax\n",
        "3. **Causal masking** is essential for text generation\n",
        "4. **Q, K, V** are different projections of the same input\n",
        "5. **Output** is a weighted sum of values\n",
        "\n",
        "---\n",
        "\n",
        "**Next: 04_multihead_attention.ipynb** - Multiple attention heads for richer representations!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Multi-Head Attention\n",
        "\n",
        "## Why Multiple Heads?\n",
        "\n",
        "A single attention head can only focus on one type of relationship at a time. Multi-head attention runs **multiple attention operations in parallel**, each learning different patterns:\n",
        "\n",
        "- One head might focus on **syntactic relationships** (subject-verb)\n",
        "- Another on **semantic similarity** (synonyms)\n",
        "- Another on **positional proximity** (nearby words)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Multi-Head Attention Architecture\n",
        "\n",
        "```\n",
        "Input X (seq_len, d_model)\n",
        "    |\n",
        "    +---> Head 1: attention(X @ W_Q1, X @ W_K1, X @ W_V1)\n",
        "    |\n",
        "    +---> Head 2: attention(X @ W_Q2, X @ W_K2, X @ W_V2)\n",
        "    |\n",
        "    +---> Head 3: attention(X @ W_Q3, X @ W_K3, X @ W_V3)\n",
        "    |\n",
        "    ...\n",
        "    |\n",
        "    +---> Head h: attention(X @ W_Qh, X @ W_Kh, X @ W_Vh)\n",
        "    |\n",
        "    v\n",
        "Concatenate all heads: [Head1 ; Head2 ; ... ; Head_h]\n",
        "    |\n",
        "    v\n",
        "Final projection: Concat @ W_O\n",
        "    |\n",
        "    v\n",
        "Output (seq_len, d_model)\n",
        "```\n",
        "\n",
        "### Key Insight: Smaller Dimensions Per Head\n",
        "\n",
        "If we have 8 heads and d_model=512:\n",
        "- Each head works with d_k = d_v = 512/8 = 64 dimensions\n",
        "- Total computation stays the same as single-head with 512 dimensions\n",
        "- But we get **8 different attention patterns**!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"Single-head attention from previous notebook.\"\"\"\n",
        "    d_k = K.shape[-1]\n",
        "    scores = Q @ K.T / np.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scores = np.where(mask == 0, -1e9, scores)\n",
        "    \n",
        "    weights = softmax(scores, axis=-1)\n",
        "    output = weights @ V\n",
        "    return output, weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    \"\"\"\n",
        "    Multi-Head Attention implemented from scratch.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        \"\"\"\n",
        "        d_model: Total dimension of the model\n",
        "        num_heads: Number of attention heads\n",
        "        \"\"\"\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Dimension per head\n",
        "        \n",
        "        # Initialize weight matrices for each head\n",
        "        # We could use separate matrices, but it's more efficient to use one big matrix\n",
        "        # and then split it into heads\n",
        "        self.W_Q = np.random.randn(d_model, d_model) * 0.1\n",
        "        self.W_K = np.random.randn(d_model, d_model) * 0.1\n",
        "        self.W_V = np.random.randn(d_model, d_model) * 0.1\n",
        "        self.W_O = np.random.randn(d_model, d_model) * 0.1\n",
        "    \n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, d_k).\n",
        "        \n",
        "        Input: (seq_len, d_model)\n",
        "        Output: (num_heads, seq_len, d_k)\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[0]\n",
        "        # Reshape: (seq_len, d_model) -> (seq_len, num_heads, d_k)\n",
        "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
        "        # Transpose: (seq_len, num_heads, d_k) -> (num_heads, seq_len, d_k)\n",
        "        return x.transpose(1, 0, 2)\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        Reverse of split_heads.\n",
        "        \n",
        "        Input: (num_heads, seq_len, d_k)\n",
        "        Output: (seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # Transpose: (num_heads, seq_len, d_k) -> (seq_len, num_heads, d_k)\n",
        "        x = x.transpose(1, 0, 2)\n",
        "        seq_len = x.shape[0]\n",
        "        # Reshape: (seq_len, num_heads, d_k) -> (seq_len, d_model)\n",
        "        return x.reshape(seq_len, self.d_model)\n",
        "    \n",
        "    def forward(self, X, mask=None):\n",
        "        \"\"\"\n",
        "        Compute multi-head attention.\n",
        "        \n",
        "        X: Input (seq_len, d_model)\n",
        "        mask: Optional attention mask\n",
        "        \n",
        "        Returns: output (seq_len, d_model), attention_weights (num_heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        # Step 1: Linear projections\n",
        "        Q = X @ self.W_Q  # (seq_len, d_model)\n",
        "        K = X @ self.W_K\n",
        "        V = X @ self.W_V\n",
        "        \n",
        "        # Step 2: Split into multiple heads\n",
        "        Q = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "        \n",
        "        # Step 3: Apply attention to each head\n",
        "        head_outputs = []\n",
        "        attention_weights = []\n",
        "        \n",
        "        for i in range(self.num_heads):\n",
        "            output, weights = scaled_dot_product_attention(\n",
        "                Q[i], K[i], V[i], mask\n",
        "            )\n",
        "            head_outputs.append(output)\n",
        "            attention_weights.append(weights)\n",
        "        \n",
        "        # Stack outputs: list of (seq_len, d_k) -> (num_heads, seq_len, d_k)\n",
        "        head_outputs = np.stack(head_outputs, axis=0)\n",
        "        attention_weights = np.stack(attention_weights, axis=0)\n",
        "        \n",
        "        # Step 4: Combine heads\n",
        "        concat_output = self.combine_heads(head_outputs)  # (seq_len, d_model)\n",
        "        \n",
        "        # Step 5: Final linear projection\n",
        "        output = concat_output @ self.W_O  # (seq_len, d_model)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# Test our implementation\n",
        "d_model = 32\n",
        "num_heads = 4\n",
        "seq_len = 6\n",
        "\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "X = np.random.randn(seq_len, d_model)\n",
        "\n",
        "output, attention_weights = mha.forward(X)\n",
        "\n",
        "print(f\"Input shape: {X.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "print(f\"\\nWe have {num_heads} attention patterns, each of size ({seq_len}, {seq_len})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize attention patterns from each head\n",
        "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"mat\", \".\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "for i in range(4):\n",
        "    ax = axes[i]\n",
        "    im = ax.imshow(attention_weights[i], cmap='Blues')\n",
        "    ax.set_title(f'Head {i+1}')\n",
        "    ax.set_xticks(range(len(sentence)))\n",
        "    ax.set_yticks(range(len(sentence)))\n",
        "    ax.set_xticklabels(sentence, rotation=45)\n",
        "    ax.set_yticklabels(sentence)\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('From')\n",
        "    ax.set_xlabel('To')\n",
        "\n",
        "plt.suptitle('Different Heads Learn Different Patterns', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice: Each head has learned a different attention pattern!\")\n",
        "print(\"In a trained model, these patterns would be meaningful.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Different Heads Learn\n",
        "\n",
        "In trained models, researchers have found that different heads specialize:\n",
        "\n",
        "| Head Type | What It Attends To |\n",
        "|-----------|-------------------|\n",
        "| Positional | Adjacent tokens (n-1, n+1) |\n",
        "| Syntactic | Subject-verb pairs |\n",
        "| Semantic | Related concepts |\n",
        "| Delimiter | Punctuation, sentence boundaries |\n",
        "| Rare token | Unusual or important words |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate different specialized heads\n",
        "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
        "\n",
        "sentence = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \".\"]\n",
        "n = len(sentence)\n",
        "\n",
        "# Head 1: Positional (attend to previous token)\n",
        "positional = np.zeros((n, n))\n",
        "for i in range(n):\n",
        "    if i > 0:\n",
        "        positional[i, i-1] = 0.7\n",
        "    positional[i, i] = 0.3\n",
        "ax = axes[0, 0]\n",
        "im = ax.imshow(positional, cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_title('Positional Head\\n(Previous token)', fontsize=12)\n",
        "ax.set_xticks(range(n))\n",
        "ax.set_yticks(range(n))\n",
        "ax.set_xticklabels(sentence, rotation=45)\n",
        "ax.set_yticklabels(sentence)\n",
        "\n",
        "# Head 2: Syntactic (subject-verb relationship)\n",
        "syntactic = np.eye(n) * 0.1\n",
        "syntactic[4, 0] = 0.6  # \"jumps\" attends to \"The\" (article of subject)\n",
        "syntactic[4, 3] = 0.8  # \"jumps\" attends to \"fox\" (subject)\n",
        "syntactic = syntactic / syntactic.sum(axis=1, keepdims=True)\n",
        "ax = axes[0, 1]\n",
        "ax.imshow(syntactic, cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_title('Syntactic Head\\n(Subject-verb)', fontsize=12)\n",
        "ax.set_xticks(range(n))\n",
        "ax.set_yticks(range(n))\n",
        "ax.set_xticklabels(sentence, rotation=45)\n",
        "ax.set_yticklabels(sentence)\n",
        "\n",
        "# Head 3: Semantic (similar concepts)\n",
        "semantic = np.eye(n) * 0.2\n",
        "semantic[2, 1] = 0.5  # \"brown\" attends to \"quick\" (both adjectives)\n",
        "semantic[1, 2] = 0.5  # vice versa\n",
        "ax = axes[0, 2]\n",
        "ax.imshow(semantic, cmap='Blues', vmin=0, vmax=0.7)\n",
        "ax.set_title('Semantic Head\\n(Similar types)', fontsize=12)\n",
        "ax.set_xticks(range(n))\n",
        "ax.set_yticks(range(n))\n",
        "ax.set_xticklabels(sentence, rotation=45)\n",
        "ax.set_yticklabels(sentence)\n",
        "\n",
        "# Head 4: Beginning-of-sequence\n",
        "bos = np.zeros((n, n))\n",
        "bos[:, 0] = 0.7\n",
        "bos += np.eye(n) * 0.3\n",
        "bos = bos / bos.sum(axis=1, keepdims=True)\n",
        "ax = axes[1, 0]\n",
        "ax.imshow(bos, cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_title('BOS Head\\n(First token)', fontsize=12)\n",
        "ax.set_xticks(range(n))\n",
        "ax.set_yticks(range(n))\n",
        "ax.set_xticklabels(sentence, rotation=45)\n",
        "ax.set_yticklabels(sentence)\n",
        "\n",
        "# Head 5: Delimiter\n",
        "delimiter = np.eye(n) * 0.2\n",
        "delimiter[:, -1] = 0.6  # Everyone attends to period\n",
        "ax = axes[1, 1]\n",
        "ax.imshow(delimiter, cmap='Blues', vmin=0, vmax=0.8)\n",
        "ax.set_title('Delimiter Head\\n(Punctuation)', fontsize=12)\n",
        "ax.set_xticks(range(n))\n",
        "ax.set_yticks(range(n))\n",
        "ax.set_xticklabels(sentence, rotation=45)\n",
        "ax.set_yticklabels(sentence)\n",
        "\n",
        "# Head 6: Uniform (background)\n",
        "uniform = np.ones((n, n)) / n\n",
        "ax = axes[1, 2]\n",
        "ax.imshow(uniform, cmap='Blues', vmin=0, vmax=0.3)\n",
        "ax.set_title('Uniform Head\\n(General context)', fontsize=12)\n",
        "ax.set_xticks(range(n))\n",
        "ax.set_yticks(range(n))\n",
        "ax.set_xticklabels(sentence, rotation=45)\n",
        "ax.set_yticklabels(sentence)\n",
        "\n",
        "plt.suptitle('Specialized Attention Patterns (Simulated)', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Math: Why It Works\n",
        "\n",
        "### Single Head\n",
        "```\n",
        "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
        "```\n",
        "\n",
        "### Multi-Head\n",
        "```\n",
        "MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W_O\n",
        "\n",
        "where head_i = Attention(Q W_Qi, K W_Ki, V W_Vi)\n",
        "```\n",
        "\n",
        "### Computational Cost\n",
        "\n",
        "For input of size (seq_len, d_model):\n",
        "\n",
        "| Operation | Single Head | Multi-Head (h heads) |\n",
        "|-----------|-------------|---------------------|\n",
        "| Q, K, V projections | O(d_model^2) | O(d_model^2) |\n",
        "| Attention per head | O(seq_len^2 * d_model) | O(seq_len^2 * d_k) * h |\n",
        "| Output projection | O(d_model^2) | O(d_model^2) |\n",
        "\n",
        "Since d_k = d_model / h, the total cost is the **same**!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate computational equivalence\n",
        "d_model = 512\n",
        "seq_len = 100\n",
        "\n",
        "# Single head\n",
        "single_head_attention_ops = seq_len * seq_len * d_model\n",
        "single_head_total = 3 * d_model * d_model + single_head_attention_ops + d_model * d_model\n",
        "\n",
        "# Multi-head with h=8\n",
        "h = 8\n",
        "d_k = d_model // h\n",
        "multi_head_attention_ops = h * (seq_len * seq_len * d_k)\n",
        "multi_head_total = 3 * d_model * d_model + multi_head_attention_ops + d_model * d_model\n",
        "\n",
        "print(f\"d_model = {d_model}, seq_len = {seq_len}, num_heads = {h}\")\n",
        "print(f\"\\nSingle head attention operations: {single_head_attention_ops:,}\")\n",
        "print(f\"Multi-head attention operations:  {multi_head_attention_ops:,}\")\n",
        "print(f\"\\nThey're equal! {single_head_attention_ops == multi_head_attention_ops}\")\n",
        "print(f\"\\nBut multi-head gives us {h} different attention patterns.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Efficient Implementation with Batched Operations\n",
        "\n",
        "In practice, we don't loop over heads - we use batched matrix operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientMultiHeadAttention:\n",
        "    \"\"\"\n",
        "    Efficient Multi-Head Attention using batched operations.\n",
        "    No explicit loops over heads!\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Combined weight matrices\n",
        "        self.W_Q = np.random.randn(d_model, d_model) * 0.1\n",
        "        self.W_K = np.random.randn(d_model, d_model) * 0.1\n",
        "        self.W_V = np.random.randn(d_model, d_model) * 0.1\n",
        "        self.W_O = np.random.randn(d_model, d_model) * 0.1\n",
        "    \n",
        "    def forward(self, X, mask=None):\n",
        "        seq_len = X.shape[0]\n",
        "        \n",
        "        # Project to Q, K, V\n",
        "        Q = X @ self.W_Q  # (seq_len, d_model)\n",
        "        K = X @ self.W_K\n",
        "        V = X @ self.W_V\n",
        "        \n",
        "        # Reshape for multi-head: (seq_len, d_model) -> (num_heads, seq_len, d_k)\n",
        "        Q = Q.reshape(seq_len, self.num_heads, self.d_k).transpose(1, 0, 2)\n",
        "        K = K.reshape(seq_len, self.num_heads, self.d_k).transpose(1, 0, 2)\n",
        "        V = V.reshape(seq_len, self.num_heads, self.d_k).transpose(1, 0, 2)\n",
        "        \n",
        "        # Batched attention: (num_heads, seq_len, d_k) @ (num_heads, d_k, seq_len)\n",
        "        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(self.d_k)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = np.where(mask == 0, -1e9, scores)\n",
        "        \n",
        "        attention_weights = softmax(scores, axis=-1)\n",
        "        \n",
        "        # (num_heads, seq_len, seq_len) @ (num_heads, seq_len, d_k)\n",
        "        context = np.matmul(attention_weights, V)\n",
        "        \n",
        "        # Reshape back: (num_heads, seq_len, d_k) -> (seq_len, d_model)\n",
        "        context = context.transpose(1, 0, 2).reshape(seq_len, self.d_model)\n",
        "        \n",
        "        # Final projection\n",
        "        output = context @ self.W_O\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Compare both implementations\n",
        "efficient_mha = EfficientMultiHeadAttention(d_model=32, num_heads=4)\n",
        "X_test = np.random.randn(6, 32)\n",
        "\n",
        "output_efficient, weights_efficient = efficient_mha.forward(X_test)\n",
        "\n",
        "print(\"Efficient implementation:\")\n",
        "print(f\"  Output shape: {output_efficient.shape}\")\n",
        "print(f\"  Weights shape: {weights_efficient.shape}\")\n",
        "print(\"\\nNo loops over heads - all done with batched matrix operations!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Multi-Head Attention\n",
        "\n",
        "### Architecture\n",
        "```\n",
        "Input X\n",
        "    |\n",
        "    v\n",
        "[Q = X @ W_Q]  [K = X @ W_K]  [V = X @ W_V]\n",
        "    |               |               |\n",
        "    v               v               v\n",
        "Split into h heads (each with d_k = d_model/h dimensions)\n",
        "    |\n",
        "    v\n",
        "Run h parallel attention operations\n",
        "    |\n",
        "    v\n",
        "Concatenate all heads\n",
        "    |\n",
        "    v\n",
        "Output projection (W_O)\n",
        "    |\n",
        "    v\n",
        "Output (same shape as input)\n",
        "```\n",
        "\n",
        "### Key Benefits\n",
        "\n",
        "1. **Multiple perspectives**: Each head can learn different relationships\n",
        "2. **No extra cost**: Same computation as single-head with full dimension\n",
        "3. **Richer representations**: Combines information from all heads\n",
        "4. **Specialization**: Different heads naturally specialize\n",
        "\n",
        "### Typical Configurations\n",
        "\n",
        "| Model | d_model | num_heads | d_k |\n",
        "|-------|---------|-----------|-----|\n",
        "| Transformer Base | 512 | 8 | 64 |\n",
        "| Transformer Large | 1024 | 16 | 64 |\n",
        "| GPT-2 Small | 768 | 12 | 64 |\n",
        "| GPT-3 | 12288 | 96 | 128 |\n",
        "\n",
        "---\n",
        "\n",
        "**Next: 05_transformer_block.ipynb** - Assembling the complete Transformer block!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
